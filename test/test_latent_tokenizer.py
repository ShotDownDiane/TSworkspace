#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LatentTSTokenizer åŠŸèƒ½æµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬æµ‹è¯•åŸºäº VQ-VAE çš„æ—¶é—´åºåˆ—åˆ†è¯å™¨çš„å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å•ä¸ªåºåˆ—çš„ç¼–ç /è§£ç 
- æ‰¹é‡åºåˆ—çš„ç¼–ç /è§£ç 
- tokenize/detokenize åŠŸèƒ½
- é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tsflow.module.vqvae import VQVAE
from tsflow.tokenizer.latent_tokenizer import LatentTSTokenizer


def create_test_vqvae_model() -> VQVAE:
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ VQ-VAE æ¨¡å‹"""
    vqvae_config = {
        'block_hidden_size': 128,
        'num_residual_layers': 2,
        'res_hidden_size': 32,
        'embedding_dim': 64,
        'num_embeddings': 512,
        'commitment_cost': 0.25,
        'compression_factor': 4
    }
    
    model = VQVAE(vqvae_config)
    model.eval()
    return model


def create_test_config() -> Dict[str, Any]:
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    class Config:
        def __init__(self):
            self.special_tokens = {
                '<PAD>': 0,
                '<START>': 1,
                '<END>': 2,
                '<UNK>': 3
            }
    
    return Config()


def generate_test_data(batch_size: int = 4, seq_len: int = 256) -> torch.Tensor:
    """ç”Ÿæˆæµ‹è¯•æ—¶é—´åºåˆ—æ•°æ®"""
    # ç”Ÿæˆå¤šç§ç±»å‹çš„æ—¶é—´åºåˆ—
    data = []
    
    for i in range(batch_size):
        t = torch.linspace(0, 4*np.pi, seq_len)
        
        if i == 0:
            # æ­£å¼¦æ³¢
            series = torch.sin(t) + 0.1 * torch.randn(seq_len)
        elif i == 1:
            # ä½™å¼¦æ³¢
            series = torch.cos(t) + 0.1 * torch.randn(seq_len)
        elif i == 2:
            # çº¿æ€§è¶‹åŠ¿ + å™ªå£°
            series = 0.5 * t + torch.randn(seq_len)
        else:
            # éšæœºæ¸¸èµ°
            series = torch.cumsum(torch.randn(seq_len) * 0.1, dim=0)
            
        data.append(series)
    
    return torch.stack(data)


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•åŸºæœ¬åŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    print(f"âœ“ åˆ›å»º LatentTSTokenizer")
    print(f"  - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"  - å‹ç¼©å› å­: {tokenizer.compression_factor}")
    print(f"  - è®¾å¤‡: {tokenizer.device}")
    
    # è·å–å‹ç¼©ä¿¡æ¯
    info = tokenizer.get_compression_info()
    print(f"âœ“ å‹ç¼©ä¿¡æ¯:")
    for key, value in info.items():
        print(f"  - {key}: {value}")
    
    print()


def test_single_sequence_processing():
    """æµ‹è¯•å•ä¸ªåºåˆ—å¤„ç†"""
    print("ğŸ”§ æµ‹è¯•å•ä¸ªåºåˆ—å¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    # ç”Ÿæˆå•ä¸ªæµ‹è¯•åºåˆ—
    single_series = generate_test_data(1, 256)[0]  # (256,)
    print(f"âœ“ ç”Ÿæˆæµ‹è¯•åºåˆ—: {single_series.shape}")
    print(f"  - æ•°æ®èŒƒå›´: [{single_series.min():.3f}, {single_series.max():.3f}]")
    
    # æµ‹è¯• encode/decode
    print(f"\nğŸ” æµ‹è¯• encode/decode:")
    encoded = tokenizer.encode(single_series)
    print(f"  - ç¼–ç å½¢çŠ¶: {encoded.shape}")
    
    decoded = tokenizer.decode(encoded)
    print(f"  - è§£ç å½¢çŠ¶: {decoded.shape}")
    
    # è®¡ç®—é‡æ„è¯¯å·®
    mse = torch.mean((single_series - decoded.squeeze()) ** 2).item()
    print(f"  - é‡æ„è¯¯å·® (MSE): {mse:.6f}")
    
    # æµ‹è¯• tokenize/detokenize
    print(f"\nğŸ” æµ‹è¯• tokenize/detokenize:")
    tokens = tokenizer.tokenize(single_series)
    print(f"  - Token æ•°é‡: {len(tokens)}")
    print(f"  - Token èŒƒå›´: [{min(tokens)}, {max(tokens)}]")
    print(f"  - å‰10ä¸ªtokens: {tokens[:10]}")
    
    reconstructed = tokenizer.detokenize(tokens)
    print(f"  - é‡æ„å½¢çŠ¶: {reconstructed.shape}")
    
    # è®¡ç®—é‡æ„è¯¯å·®
    mse_tokens = np.mean((single_series.numpy() - reconstructed) ** 2)
    print(f"  - Tokené‡æ„è¯¯å·® (MSE): {mse_tokens:.6f}")
    
    print()


def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("ğŸ”§ æµ‹è¯•æ‰¹é‡å¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    # ç”Ÿæˆæ‰¹é‡æµ‹è¯•æ•°æ®
    batch_data = generate_test_data(4, 256)  # (4, 256)
    print(f"âœ“ ç”Ÿæˆæ‰¹é‡æ•°æ®: {batch_data.shape}")
    
    # æµ‹è¯•æ‰¹é‡ encode/decode
    print(f"\nğŸ” æµ‹è¯•æ‰¹é‡ encode/decode:")
    encoded_batch = tokenizer.encode(batch_data)
    print(f"  - æ‰¹é‡ç¼–ç å½¢çŠ¶: {encoded_batch.shape}")
    
    decoded_batch = tokenizer.decode(encoded_batch)
    print(f"  - æ‰¹é‡è§£ç å½¢çŠ¶: {decoded_batch.shape}")
    
    # è®¡ç®—æ‰¹é‡é‡æ„è¯¯å·®
    mse_batch = torch.mean((batch_data - decoded_batch) ** 2).item()
    print(f"  - æ‰¹é‡é‡æ„è¯¯å·® (MSE): {mse_batch:.6f}")
    
    # æµ‹è¯•æ‰¹é‡ tokenize/detokenize
    print(f"\nğŸ” æµ‹è¯•æ‰¹é‡ tokenize/detokenize:")
    tokens_batch = tokenizer.batch_tokenize(batch_data)
    print(f"  - æ‰¹é‡ Token æ•°é‡: {len(tokens_batch)}")
    print(f"  - æ¯ä¸ªåºåˆ—çš„ Token æ•°é‡: {[len(tokens) for tokens in tokens_batch]}")
    
    reconstructed_batch = tokenizer.batch_detokenize(tokens_batch)
    print(f"  - æ‰¹é‡é‡æ„å½¢çŠ¶: {reconstructed_batch.shape}")
    
    # è®¡ç®—æ‰¹é‡é‡æ„è¯¯å·®
    mse_batch_tokens = np.mean((batch_data.numpy() - reconstructed_batch) ** 2)
    print(f"  - æ‰¹é‡Tokené‡æ„è¯¯å·® (MSE): {mse_batch_tokens:.6f}")
    
    print()


def test_input_formats():
    """æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼"""
    print("ğŸ”§ æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = generate_test_data(1, 128)[0]
    
    # æµ‹è¯• torch.Tensor è¾“å…¥
    print(f"âœ“ æµ‹è¯• torch.Tensor è¾“å…¥:")
    tokens_tensor = tokenizer.tokenize(test_data)
    print(f"  - Token æ•°é‡: {len(tokens_tensor)}")
    
    # æµ‹è¯• numpy è¾“å…¥
    print(f"âœ“ æµ‹è¯• numpy.ndarray è¾“å…¥:")
    test_numpy = test_data.numpy()
    tokens_numpy = tokenizer.tokenize(test_numpy)
    print(f"  - Token æ•°é‡: {len(tokens_numpy)}")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    tokens_equal = tokens_tensor == tokens_numpy
    print(f"  - ç»“æœä¸€è‡´æ€§: {tokens_equal}")
    
    # æµ‹è¯•ä¸åŒ token æ ¼å¼çš„ detokenize
    print(f"\nâœ“ æµ‹è¯•ä¸åŒ token æ ¼å¼çš„ detokenize:")
    
    # list æ ¼å¼
    reconstructed_list = tokenizer.detokenize(tokens_tensor)
    print(f"  - list è¾“å…¥é‡æ„å½¢çŠ¶: {reconstructed_list.shape}")
    
    # numpy æ ¼å¼
    tokens_np = np.array(tokens_tensor)
    reconstructed_np = tokenizer.detokenize(tokens_np)
    print(f"  - numpy è¾“å…¥é‡æ„å½¢çŠ¶: {reconstructed_np.shape}")
    
    # tensor æ ¼å¼
    tokens_torch = torch.tensor(tokens_tensor)
    reconstructed_torch = tokenizer.detokenize(tokens_torch)
    print(f"  - tensor è¾“å…¥é‡æ„å½¢çŠ¶: {reconstructed_torch.shape}")
    
    print()


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("ğŸ”§ æµ‹è¯•é”™è¯¯å¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    # æµ‹è¯•æ— æ•ˆè¾“å…¥ç±»å‹
    print(f"âœ“ æµ‹è¯•æ— æ•ˆè¾“å…¥ç±»å‹:")
    try:
        tokenizer.tokenize("invalid_input")
        print("  - âŒ åº”è¯¥æŠ›å‡º TypeError")
    except TypeError as e:
        print(f"  - âœ“ æ­£ç¡®æ•è· TypeError: {str(e)[:50]}...")
    
    # æµ‹è¯•æ— æ•ˆç»´åº¦
    print(f"âœ“ æµ‹è¯•æ— æ•ˆç»´åº¦:")
    try:
        invalid_data = torch.randn(2, 3, 4, 5)  # 4D tensor
        tokenizer.tokenize(invalid_data)
        print("  - âŒ åº”è¯¥æŠ›å‡º ValueError")
    except ValueError as e:
        print(f"  - âœ“ æ­£ç¡®æ•è· ValueError: {str(e)[:50]}...")
    
    # æµ‹è¯•æ‰¹é‡ tokenize çš„å•åºåˆ—é™åˆ¶
    print(f"âœ“ æµ‹è¯•æ‰¹é‡æ•°æ®çš„å•åºåˆ—é™åˆ¶:")
    try:
        batch_data = torch.randn(3, 128)  # 3ä¸ªåºåˆ—
        tokenizer.tokenize(batch_data)
        print("  - âŒ åº”è¯¥æŠ›å‡º ValueError")
    except ValueError as e:
        print(f"  - âœ“ æ­£ç¡®æ•è· ValueError: {str(e)[:50]}...")
    
    # æµ‹è¯•ç©º tokens_batch
    print(f"âœ“ æµ‹è¯•ç©º tokens_batch:")
    try:
        tokenizer.batch_detokenize([])
        print("  - âŒ åº”è¯¥æŠ›å‡º ValueError")
    except ValueError as e:
        print(f"  - âœ“ æ­£ç¡®æ•è· ValueError: {str(e)[:50]}...")
    
    print()


def visualize_reconstruction():
    """å¯è§†åŒ–é‡æ„ç»“æœ"""
    print("ğŸ”§ å¯è§†åŒ–é‡æ„ç»“æœ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model = create_test_vqvae_model()
    config = create_test_config()
    tokenizer = LatentTSTokenizer(config, model)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = generate_test_data(2, 256)
    
    # è¿›è¡Œç¼–ç è§£ç 
    tokens_batch = tokenizer.batch_tokenize(test_data)
    reconstructed = tokenizer.batch_detokenize(tokens_batch)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('LatentTSTokenizer Reconstruction Results', fontsize=16)
    
    for i in range(2):
        # åŸå§‹ä¿¡å·
        axes[i, 0].plot(test_data[i].numpy(), 'b-', label='Original', alpha=0.8)
        axes[i, 0].set_title(f'Sample {i+1}: Original Signal')
        axes[i, 0].set_xlabel('Time Steps')
        axes[i, 0].set_ylabel('Value')
        axes[i, 0].legend()
        axes[i, 0].grid(True, alpha=0.3)
        
        # é‡æ„ä¿¡å·
        axes[i, 1].plot(test_data[i].numpy(), 'b-', label='Original', alpha=0.6)
        axes[i, 1].plot(reconstructed[i], 'r--', label='Reconstructed', alpha=0.8)
        axes[i, 1].set_title(f'Sample {i+1}: Original vs Reconstructed')
        axes[i, 1].set_xlabel('Time Steps')
        axes[i, 1].set_ylabel('Value')
        axes[i, 1].legend()
        axes[i, 1].grid(True, alpha=0.3)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºè¯¯å·®
        mse = np.mean((test_data[i].numpy() - reconstructed[i]) ** 2)
        axes[i, 1].text(0.02, 0.98, f'MSE: {mse:.6f}', 
                       transform=axes[i, 1].transAxes, 
                       verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig('../latent_tokenizer_test_results.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ ä¿å­˜å¯è§†åŒ–ç»“æœåˆ° '../latent_tokenizer_test_results.png'")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š é‡æ„ç»Ÿè®¡ä¿¡æ¯:")
    for i in range(2):
        mse = np.mean((test_data[i].numpy() - reconstructed[i]) ** 2)
        mae = np.mean(np.abs(test_data[i].numpy() - reconstructed[i]))
        print(f"  - æ ·æœ¬ {i+1}: MSE={mse:.6f}, MAE={mae:.6f}")
        print(f"    Tokenæ•°é‡: {len(tokens_batch[i])}, å‹ç¼©æ¯”: {test_data.shape[1]/len(tokens_batch[i]):.1f}x")
    
    print()


def run_comprehensive_test():
    """è¿è¡Œå®Œæ•´æµ‹è¯•"""
    print("ğŸš€ LatentTSTokenizer å®Œæ•´åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    print()
    
    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_basic_functionality()
        test_single_sequence_processing()
        test_batch_processing()
        test_input_formats()
        test_error_handling()
        visualize_reconstruction()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        print()
        print("ä¸»è¦åŠŸèƒ½éªŒè¯:")
        print("âœ“ åŸºæœ¬åŠŸèƒ½åˆå§‹åŒ–")
        print("âœ“ å•ä¸ªåºåˆ—ç¼–ç /è§£ç ")
        print("âœ“ æ‰¹é‡åºåˆ—å¤„ç†")
        print("âœ“ å¤šç§è¾“å…¥æ ¼å¼æ”¯æŒ")
        print("âœ“ é”™è¯¯å¤„ç†æœºåˆ¶")
        print("âœ“ å¯è§†åŒ–é‡æ„ç»“æœ")
        print()
        print("LatentTSTokenizer å·²æˆåŠŸé›†æˆ VQ-VAE çš„ç¼–ç è§£ç èƒ½åŠ›ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_comprehensive_test()